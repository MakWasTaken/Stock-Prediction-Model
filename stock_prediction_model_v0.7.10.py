# -*- coding: utf-8 -*-
"""stockAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WhLKPWZ0sanINbqZScb6OF6RPHQ9QM23
"""

# importing yahoo library
import yfinance as yf

# importing nvidia dataset
stock_data = yf.download('^TNX',start='2014-06-28',end='2024-07-06')
stock_data

# converting data into machine code (1s and 0s) for AI to interpret
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0,1))

# tranforms and reshape the data into reading Closing prices only
scaled_data = scaler.fit_transform(stock_data["Close"].values.reshape(-1,1))

# this will use sequential model, need to sequence the data out based on certain time step
# so the model can pick up on the pattern as it goes on

# the idea is we want the next values that the model is prediciting to be very much based on the previous values
# sequential data is essentially data where order MATTERS
# stock is a perfect exaple of it cus it changes everyday

import numpy as np

def create_dataset(data,time_step):
  x,y=[],[]
  for i in range(len(data)-time_step-1):
    # x is going to be all the values leading up to Y value
    x.append(data[i:(i + time_step),0])
    y.append(data[i + time_step,0])

  return np.array(x),np.array(y)

# days
# the window/ interval the model is looking at, every 100 days throughout 10 years
time_step=100

x,y = create_dataset(scaled_data, time_step)

# making a training set

# percentage of dataset for training
train_size= 0.8;

# dividing parameters into training and testing for X and Y values
x_train,x_test=x[:int(x.shape[0]*train_size)],x[int(x.shape[0]*train_size):]
y_train,y_test=y[:int(y.shape[0]*train_size)],y[int(y.shape[0]*train_size):]

!pip install --no-deps scikeras
!pip install scikeras

from keras.models import Sequential
from keras.layers import Dense, LSTM
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import cross_val_score, KFold

# Define a function to build the model
def build_model():
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=(time_step, 1)))
    model.add(LSTM(64))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Wrap the model using kerasRegressor
model = KerasRegressor(build_fn=build_model, epochs=10, batch_size=32,shuffle=True)

# Cross-validation
cv = KFold(n_splits=3)
results = cross_val_score(model, x_train, y_train, cv=cv)
print("Cross-validation scores: ", results)
print("Mean cross-validation score: ", results.mean())

# testing model on new data to check for overfitting

# Train the final model on the entire training set
final_model = build_model()
final_model.fit(x_train, y_train, epochs=10,batch_size=32, shuffle=True)


# Evaluate the final model on the test set
test_loss=final_model.evaluate(x_test, y_test,batch_size=32)
print("Test loss: ", test_loss)

# graphing stock price predictions (current ones, still testing)

# step 1: make predictions

# shows model's prediction of what the stock value is going to be
# compared to the actual stock value

predictions = final_model.predict(x_test)

# we scaled our data, so we need to un-scale it to plot the values
# this will give the actual value of the stock and not just the scaled number
predictions = scaler.inverse_transform(predictions)

original_data=stock_data["Close"].values
# creating size of an NP array
predicted_data = np.empty_like(original_data)
# making them all nan
predicted_data[:]=np.nan
# rehspaing the predictions, putting the actual predictions into matplotlib
predicted_data[-len(predictions):] = predictions.reshape(-1)

import matplotlib.pyplot as plt

plt.plot(original_data)
plt.plot(predicted_data)
plt.xlabel('days')
plt.ylabel('price')
plt.title('Predicted (Orange) vs Original (Blue) Data')

predictions=predictions.reshape(-1)
predictions.shape

from sklearn.metrics import r2_score

# Assuming predictions is the array of predicted values
predictions = final_model.predict(x_test)

# Calculate R^2 score
r2 = r2_score(y_test, predictions)

# Print R^2 score
print("R^2 Score:", r2)

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)

# Print metrics
print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)

# predicting future prices
# setting the prediction time too high can make the model "hallucinate"
new_predictions = final_model.predict(x_test[-90:])

# unscale the data
new_predictions = scaler.inverse_transform(new_predictions)

# combine the predicted data with the new predictions
# to see beyond the blue line from graph above
predicted_data = np.append(predicted_data,new_predictions)

new_predictions=new_predictions.reshape(-1)

import pandas as pd
from datetime import datetime, timedelta


# Create a date range starting from July 3rd, 2024
start_date = datetime(2024, 7, 5)
date_range = [start_date + timedelta(days=i) for i in range(len(new_predictions))]

# Create a DataFrame
df = pd.DataFrame({
    'Date': date_range,
    'Prediction': new_predictions
})

# Save the DataFrame to a CSV file
df.to_excel('predictions_with_dates.xlsx', index=False)

print(df)

plt.plot(original_data)
plt.plot(predicted_data)
plt.xlabel('days')
plt.ylabel('price')
plt.title('JPY=X: Predicted (Orange) vs Original (Blue) Data')

# ignore this cell

# from keras.models import Sequential
# from keras.layers import Dense, LSTM
# from keras.optimizers import Adam
# from scikeras.wrappers import KerasRegressor
# from sklearn.model_selection import GridSearchCV
# import numpy as np

# # Define the model
# def build_model(lstm_units, dense_units, learning_rate):
#     model = Sequential()
#     model.add(LSTM(lstm_units, return_sequences=True, input_shape=(time_step, 1)))
#     model.add(LSTM(lstm_units))
#     model.add(Dense(dense_units, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])
#     return model

# # Wrap the model using KerasRegressor
# model = KerasRegressor(model=build_model, lstm_units=32, dense_units=32, learning_rate=0.001, epochs=10, batch_size=32, verbose=0)

# # Define the hyperparameter grid
# param_grid = {
#     'lstm_units': [32, 64, 128],
#     'dense_units': [32, 64, 128],
#     'learning_rate': [0.001, 0.01, 0.1],
#     'epochs': [10, 20],
#     'batch_size': [16, 32, 64]
# }

# # Implement Grid Search
# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, scoring='neg_mean_squared_error')
# grid_result = grid.fit(x_train, y_train)

# # Print the best parameters and best score
# print("Best parameters found: ", grid_result.best_params_)
# print("Best score: ", grid_result.best_score_)

# # Train the final model with the best hyperparameters
# best_params = grid_result.best_params_

# final_model = build_model(lstm_units=best_params['lstm_units'],
#                           dense_units=best_params['dense_units'],
#                           learning_rate=best_params['learning_rate'])

# final_model.fit(x_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], shuffle=True)

# # Evaluate the final model on the test set
# test_loss = final_model.evaluate(x_test, y_test)
# print("Test loss: ", test_loss)